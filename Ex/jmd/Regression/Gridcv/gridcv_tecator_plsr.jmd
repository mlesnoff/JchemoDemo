---
title: gridcv - tecator - Plsr
weave_options:
  error: true
  wrap: false
  term: false
  out_width: 60%
  out_height: 30%
---

```julia
using Jchemo, JchemoData
using JLD2, CairoMakie
```

#### Data importation

```julia
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, "data/tecator.jld2") 
@load db dat
@names dat
```

```julia
X = dat.X
@head X 
```

```julia 
Y = dat.Y 
@head Y
``` 

```julia 
typ = Y.typ
tab(typ)
```

```julia
wlst = names(X)
wl = parse.(Float64, wlst) 
#plotsp(X, wl; xlabel = "Wavelength (nm)", ylabel = "Absorbance").f
```

**Preprocessing**

```julia
model1 = snv()
model2 = savgol(npoint = 15, deriv = 2, degree = 3)
model = pip(model1, model2)
fit!(model, X)
@head Xp = transf(model, X) 
#plotsp(Xp, wl; xlabel = "Wavelength (nm)", ylabel = "Absorbance").f
```

#### Split Tot to Train/Test

The model is fitted on **Train**, and the generalization error is estimated on **Test**. In this example, **Train** is 
already defined in variable `typ` of the dataset, and **Test** is defined by the remaining samples. But **Tot** could 
also be split *a posteriori*, for instance by sampling (random, systematic or any other designs). See for instance 
functions `samprand`, `sampsys`, etc.

```julia
s = typ .== "train"
Xtrain = Xp[s, :] 
Ytrain = Y[s, :]
Xtest = rmrow(Xp, s)
Ytest = rmrow(Y, s)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
ntot = ntrain + ntest
(ntot = ntot, ntrain, ntest)
```

**Working response y**

```julia
namy = names(Y)[1:3]
j = 2  
nam = namy[j]    # work on the j-th y-variable
ytrain = Ytrain[:, nam]
ytest = Ytest[:, nam]
```

#### CV-Segments for model tuning

Two methods can be used to build the CV segments within **Train** (for the same total number of segments, these two 
methods return close resuts):

- (1) **Replicated K-fold CV**
    - Train is splitted in a number of K folds (segments),
    - and this split can be replicated (==> replicated K-Fold CV).

```julia
K = 3     # nb. folds (segments)
rep = 25  # nb. replications
segm = segmkf(ntrain, K; rep = rep)
```

- (2) **Replicated "Test-set" CV** 
    - Train is split to Cal/Val (e.g. Cal = 70% of Train, Val = 30% of Train), and this is replicated.

```julia
#pct = .30
#m = Int(round(pct * ntrain))
#segm = segmts(ntrain, m; rep = 30)
```

Illustration of segments:

```julia
i = 1  
segm[i]      # the K segments of replication 'i'
```

```julia
k = 1
segm[i][k]   # segment 'k' of replication 'i'
```

#### Grid-search 

The best syntax to use function `gridcv` for LV-based functions (eg. `plskern`, `kplsr`, `lwplsr`, etc.) is to
set parameter `nlv` outside argument `pars` defining the grid (in general with function `mpar`). In that case, 
the computation time is reduced [See the naïve and non-optimized syntax at the end of this script]. This is the same 
principle when definining the parameter `lb` in ridge-based functions (`rr`, `krr`, etc.).

```julia
nlv = 0:20
model = plskern()
rescv = gridcv(model, Xtrain, ytrain; segm, score = rmsep, nlv)
@names rescv 
res = rescv.res
res_rep = rescv.res_rep
```

```julia
plotgrid(res.nlv, res.y1; step = 2, xlabel = "Nb. LVs", ylabel = "RMSEP-CV").f
```

```julia
f, ax = plotgrid(res.nlv, res.y1; step = 2, xlabel = "Nb. LVs", ylabel = "RMSEP-CV")
for i = 1:rep, j = 1:K
    zres = res_rep[res_rep.rep .== i .&& res_rep.segm .== j, :]
    lines!(ax, zres.nlv, zres.y1; color = (:grey, .2))
end
lines!(ax, res.nlv, res.y1; color = :red, linewidth = 1)
f
```

**Selection of the best parameter combination**

```julia
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
```

#### Final prediction (Test) using the optimal model

```julia
model = plskern(nlv = res.nlv[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
```

**Generalization error**

```julia
rmsep(pred, ytest)
```

**Plotting predictions vs. observed data**

```julia 
plotxy(pred, ytest; size = (500, 400), color = (:red, .5), bisect = true, title = "Test set - variable $nam", 
    xlabel = "Prediction", ylabel = "Observed").f   
```

#### Additional parameters in the grid

If other parameters have to be defined in the grid, they have to be set in argument `pars` built with function `mpar`, 
such as in the example below.

```julia
pars = mpar(scal = [false; true])
nlv = 0:20
model = plskern()
res = gridcv(model, Xtrain, ytrain; segm, score = rmsep, pars, nlv).res
```

```julia
plotgrid(res.nlv, res.y1, res.scal; step = 2, xlabel = "Nb. LVs", ylabel = "RMSEP-Val").f
```

```julia
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
```

```julia
model = plskern(nlv = res.nlv[u], scal = res.scal[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
```

#### Naïve (not time-efficient) syntax to use gridcv for LV-based functions 

Parameter `nlv` can also be set in argument `pars` (wich is the generic approach to define the grid). This is strictly 
equivalent (gives the same results) but the computations are slower.

```julia
pars = mpar(nlv = 0:20)
model = plskern()
gridcv(model, Xtrain, ytrain; segm, score = rmsep, pars).res
```

```julia
pars = mpar(nlv = 0:20, scal = [false; true])
model = plskern()
gridcv(model, Xtrain, ytrain; segm, score = rmsep, pars).res
```

