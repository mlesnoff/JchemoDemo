---
title: gridscore_all_models_challenge2018.jl
weave_options:
  error: true
  wrap: true
  term: false
  #out_width: "50%"   # default
---

```julia
using Jchemo, JchemoData
using JLD2, CairoMakie
using FreqTables
```

#### Data importation

```julia
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, "data/challenge2018.jld2") 
@load db dat
pnames(dat)
```

#### Data preparation and short description

```julia
X = dat.X 
Y = dat.Y
ntot, p = size(X)
```

```julia term = true
@head X
@head Y
```

```julia
summ(Y)
```

```julia
y = Y.conc
typ = Y.typ
label = Y.label 
test = Y.test
tab(test)
```

```julia
wlst = names(X)
wl = parse.(Float64, wlst) 
```

```julia
freqtable(string.(typ, "-", Y.label))
```

```julia
freqtable(typ, test)
```

```julia
plotsp(X, wl; nsamp = 30).f
```

#### Preprocessing

```julia
mo1 = snv(centr = true, scal = true)
mo2 = savgol(npoint = 21, deriv = 2, degree = 3)
mo = pip(mo1, mo2)
fit!(mo, X)
Xp = transf(mo, X) 
```

``` julia
plotsp(Xp, wl; nsamp = 30).f
```

#### Split Tot ==> Train + Test

The model is tuned on **Train**, and the generalization error is estimated on **Test**.
Here the split of **Tot** is already provided inside the dataset (= variable `test`), 
but **Tot** could also be split *a posteriori*, for instance by sampling (random, systematic 
or any other designs). 

```julia
s = Bool.(test)
Xtrain = rmrow(Xp, s)
ytrain = rmrow(y, s)
Xtest = Xp[s, :]
ytest = y[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = ntot, ntrain, ntest)
```

#### Split Train ==> Cal + Val

The validation error (used for the grid-search tuning) is computed 
on **Val**. Here the split is built from random sampling 
(other designs are possible).

```julia
nval = 300
s = samprand(ntrain, nval)
Xcal = Xtrain[s.train, :]
ycal = ytrain[s.train]
Xval = Xtrain[s.test, :]
yval = ytrain[s.test]
ncal = ntrain - nval 
(ntot = ntot, ntrain, ntest, ncal, nval)
```

#### Plsr

```julia term = true
nlv = 0:50
pars = mpar(scal = [false; true])
length(pars[1])
mo = plskern()  
res = gridscore(mo, Xcal, ycal, Xval, yval; score = rmsep, 
    pars, nlv) 
group = string.(res.scal) 
plotgrid(res.nlv, res.y1, group; step = 2, xlabel ="Nb. LVs", 
    ylabel = "RMSEP").f
u = findall(res.y1 .== minimum(res.y1))[1]
res[u, :] 
mo = plskern(nlv = res.nlv[u], scal = res.scal[u])
fit!(mo, Xtrain, ytrain) 
pred = predict(mo, Xtest).pred 
@show rmsep(pred, ytest)
plotxy(pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = "Prediction", ylabel = "Observed (Test)").f  
```

#### Plsr-avg

```julia term = true
nlv = [0:30, 0:50, 0:70, 0:100]
pars = mpar(nlv = nlv)
length(pars[1])
mo = plsravg()
res = gridscore(mo, Xcal, ycal, Xval, yval; score = rmsep, 
    pars, verbose = false) 
u = findall(res.y1 .== minimum(res.y1))[1]
res[u, :] 
mo = plsravg(nlv = res.nlv[u])
fit!(mo, Xtrain, ytrain) 
pred = predict(mo, Xtest).pred 
@show rmsep(pred, ytest)
```

#### Rr 

```julia term = true
lb = 10.0.^(-15:.1:3)
mo = rr()
res = gridscore(mo, Xcal, ycal, Xval, yval; score = rmsep, 
    lb) 
zres = res[res.y1 .< 2, :]
zlb = round.(log.(10, zres.lb), digits = 3)
plotgrid(zlb, zres.y1; step = 2,
    xlabel ="Lambda", ylabel = "RMSEP").f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mo = rr(lb = res.lb[u]) ;
fit!(mo, Xtrain, ytrain) 
pred = predict(mo, Xtest).pred 
@show rmsep(pred, ytest)
```

#### Covselr

```julia term = true
nlv = 0:40
pars = mpar(msparse = [:hard], nvar = [1])
mo = splskern()
res = gridscore(mo, Xcal, ycal, Xval, yval; score = rmsep, 
    pars, nlv, verbose = false)
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mo = splskern(msparse = res.msparse[u], nvar = res.nvar[u], 
    nlv = res.nlv[u])
fit!(mo, Xtrain, ytrain) 
pred = predict(mo, Xtest).pred 
@show rmsep(pred, ytest)
```

#### Krr

```julia term = true
lb = 10.0.^(-15:3) 
gamma = 10.0.^(-3:5) 
pars = mpar(gamma = gamma) 
length(pars[1])
## To decrease the computation time, 
## sampling in Cal
m = 1000
s = samprand(ncal, m)
zXcal = Xcal[s.test, :]
zycal = ycal[s.test]
## End
mo = krr()
res = gridscore(mo, zXcal, zycal, Xval, yval; score = rmsep, 
    pars, lb, verbose = false)
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mo = krr(lb = res.lb[u], gamma = res.gamma[u]) 
fit!(mo, Xtrain, ytrain) 
pred = predict(mo, Xtest).pred 
@show rmsep(pred, ytest)
plotxy(pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = "Prediction", ylabel = "Observed (Test)").f  
```

#### Kplsr

```julia term = true
nlv = 0:100 
gamma = 10.0.^(-3:5)
pars = mpar(gamma = gamma)
length(pars[1])
## To decrease the computation time 
## Sampling in Cal
m = 1000
s = samprand(ncal, m)
zXcal = Xcal[s.test, :]
zycal = ycal[s.test]
## End
mo = kplsr()
res = gridscore(mo, zXcal, zycal, Xval, yval; score = rmsep,
    pars, nlv, verbose = false) 
u = findall(res.y1 .== minimum(res.y1))[1] ;
res[u, :]
mo = kplsr(nlv = res.nlv[u], gamma = res.gamma[u])
fit!(mo, Xtrain, ytrain) 
pred = predict(mo, Xtest).pred 
@show rmsep(pred, ytest)
```

#### Dkplsr

```julia term = true
nlv = 0:100 
gamma = 10.0.^(-3:5)
pars = mpar(gamma = gamma)
length(pars[1])
mo = dkplsr()
res = gridscore(mo, Xcal, ycal, Xval, yval; score = rmsep,
    pars, nlv, verbose = false) 
u = findall(res.y1 .== minimum(res.y1))[1] ;
res[u, :]
mo = dkplsr(nlv = res.nlv[u], gamma = res.gamma[u])
fit!(mo, Xtrain, ytrain) 
pred = predict(mo, Xtest).pred 
@show rmsep(pred, ytest)
```

#### Lwplsr

```julia term = true
nlvdis = [15; 25] ; metric = [:mah] 
h = [1; 2; 4; 6; Inf]
k = [150; 200; 350; 500; 1000]  
nlv = 0:20 
pars = mpar(nlvdis = nlvdis, metric = metric, 
    h = h, k = k)
length(pars[1])
mo = lwplsr()
res = gridscore(mo, Xcal, ycal, Xval, yval; score = rmsep, 
    pars, nlv, verbose = true) 
group = string.(res.nlvdis, "-", res.h, "-", res.k) 
plotgrid(res.nlv, res.y1, group; xlabel ="Nb. LVs", 
    ylabel = "RMSEP").f
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mo = lwplsr(nlvdis = res.nlvdis[u], metric = res.metric[u], 
    h = res.h[u], k = res.k[u], nlv = res.nlv[u]) 
fit!(mo, Xtrain, ytrain)
pred = Jchemo.predict(mo, Xtest).pred 
rmsep(pred, ytest)
```

```julia term = true
## A performant model 
mo = lwplsr(nlvdis = 15, metric = :mah, 
    h = 2, k = 200, nlv = 15) ;
fit!(mo, Xtrain, ytrain) 
pred = predict(mo, Xtest).pred 
rmsep(pred, ytest)
```

#### Lwplsr-avg 

```julia term = true
nlv = [0:20, 5:20, 10:20]
nlvdis = [15; 25] ; metric = [:mah] 
h = [1; 2; 4] ; k = [150; 200; 350; 500; 1000]  
pars = mpar(nlv = nlv, nlvdis = nlvdis, 
    metric = metric, h = h, k = k) 
length(pars[1])
mo = lwplsravg() 
res = gridscore(mo, Xcal, ycal, Xval, yval; score = rmsep, 
    pars, verbose = false) 
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mo = lwplsravg(nlvdis = res.nlvdis[u], metric = res.metric[u], 
    h = res.h[u], k = res.k[u], nlv = res.nlv[u]) ;
fit!(mo, Xtrain, ytrain)
pred = Jchemo.predict(mo, Xtest).pred ;
rmsep(pred, ytest)
```

```julia term = true
## A performant model
nlvdis = 15 ; metric = :mah  
h = 2 ; k = 200 
nlv = 5:20
mo = lwplsravg(; nlvdis, metric, h, k, nlv)
fit!(mo, Xtrain, ytrain) 
pred = predict(mo, Xtest).pred 
rmsep(pred, ytest)
```

#### Lwplsr on preliminary PLS scores 
## (Pipeline: only the last model is tuned)

```julia term = true
mo1 = plskern(nlv = 30)
nlvdis = [0; 15; 25] ; metric = [:eucl, :mah] 
h = [1; 2; 4; 6; Inf]
k = [150; 200; 350; 500; 1000]  
nlv = 0:20 
pars = mpar(nlvdis = nlvdis, metric = metric, 
    h = h, k = k)
length(pars[1])
mo2 = lwplsr()
mo = pip(mo1, mo2)
res = gridscore(mo, Xcal, ycal, Xval, yval; score = rmsep, 
    pars, nlv, verbose = true) 
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
mo2 = lwplsr(nlvdis = res.nlvdis[u], metric = res.metric[u], 
    h = res.h[u], k = res.k[u], nlv = res.nlv[u])
mo = pip(mo1, mo2)
fit!(mo, Xtrain, ytrain) 
pred = predict(mo, Xtest).pred 
rmsep(pred, ytest)
```

#### Knnr

```julia term = true
nlvdis = [15; 20]  ; metric = [:mah] 
h = [1; 2; 4; 6; Inf] 
k = [1; collect(5:10:100)] 
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k) 
length(pars[1])
mo = knnr()
res = gridscore(mo, Xcal, ycal, Xval, yval; score = rmsep, 
    pars, verbose = false) 
u = findall(res.y1 .== minimum(res.y1))[1]
res[u, :]
mo = knnr(nlvdis = res.nlvdis[u], metric = res.metric[u], 
    h = res.h[u], k = res.k[u])
fit!(mo, Xtrain, ytrain) 
pred = predict(mo, Xtest).pred 
rmsep(pred, ytest)
```

#### Rfr

```julia term = true
n_trees = [100]
n_subfeatures = LinRange(5, p / 2, 5)
max_depth = [6; 10; 20; 2000]
pars = mpar(n_trees = n_trees, n_subfeatures = n_subfeatures, 
    max_depth = max_depth)
length(pars[1])
res = gridscore(mo, Xcal, ycal, Xval, yval; score = rmsep, 
    pars, verbose = false) 
u = findall(res.y1 .== minimum(res.y1))[1]
res[u, :]
mo = rfr_dt(n_trees = res.n_trees[u], n_subfeatures = res.n_subfeatures[u], 
    max_depth = res.max_depth[u])
fit!(mo, Xtrain, ytrain) 
pred = predict(mo, Xtest).pred 
@show rmsep(pred, ytest)
plotxy(pred, ytest; color = (:red, .5), bisect = true, 
    xlabel = "Prediction", ylabel = "Observed (Test)").f  
```

