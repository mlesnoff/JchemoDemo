---
title: Introduction to the kNN-LWPLSR algorithmn
weave_options:
  error: true
  wrap: true
  term: false
  out_width: "60%" 
---

The note introduces how to implement the 
[kNN-LWPLSR algorithm](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/cem.3209) 
with [Julia](https://julialang.org/) package 
[Jchemo](https://github.com/mlesnoff/Jchemo.jl).
The workflow examples are illustrated with dataset 
[challenge2008](https://github.com/mlesnoff/JchemoData.jl/tree/main?tab=readme-ov-file#challenge2018) 
on near-infrared spectroscopy (NIRS). *Note:* almost all the contents of this note can be diretly translated 
to the algorithm also provided in the R package 
[rchemo](https://cran.r-project.org/web/packages/rchemo/index.html).

kNN-LWPLSR is an efficient and versatile pipeline combining nearest neighborhood selection and partial least squared 
regression ([8,9]). kNN-LWPLSR is well suited when the predictive variables (*X*) are multicollinear and when 
heterogeneity in the data generates non-linear relations between *X* and the response variables
to predict (*Y*).  

#### 1. Preliminaries

###### 1.1 NIRS regression 

NIRS is a fast and nondestructive analytical method used in many agronomic contexts, for instance to 
evaluate the nutritive quality of forages. Basically, spectral data *X* (matrix of *n* observations and 
*p* wavelengths) are collected on samples of the material to study (e.g. forages) using a spectrometer, 
and targeted response variables *Y = {y1, …, yq}* (*q* vectors of *n* observations; e.g. chemical 
compositions) are measured precisely in laboratory. Regression models of *Y* on *X* are then fitted 
and used to predict the response variables from new spectral observations. 

Spectral data are known to be highly collinear in columns and, in general, matrix *X* is ill-conditioned. 
Regularization methods have to be implemented to solve the regression problem. A very popular regularization 
used for NIRS is the partial least squares regression (PLSR) [1–3], which is one of the 
regression methods based on *latent variables* (LVs). PLSR starts by reducing the dimension of *X* 
(nb. columns) to a limited number *a << p* of orthogonal vectors *n x 1* maximizing the squared covariance 
with *Y*, and referred to as scores *T (n x a)*. Response *Y* is then regressed on scores *T* (latent variable) 
by a multiple linear regression (MLR). PLSR is in general very efficient when the relationship 
between *X* and *Y* is linear. The method is fast (in particular the *Dayal & McGregor kernel #1* algorithm [25]),
even for large data. The parameter to tune is the dimension of *T* (number of LVs).

For several years, agronomic databases (e.g. in feed, food or soils researches) tend to aggregate 
large numbers of samples of different natures or origins, bringing heterogeneity. This generates curvatures 
and/or clustering in the data that can alter the linear relation between *X* and *Y* and therefore the PLSR 
predictions. **Local PLSR** methodology is an easy approach that can turn out non-linearity in the data [4–7].  
The general principle is, for each new observation to predict (*x_new*), to do a pre-selection of *k* nearest 
neighbors of the observation (the *kNN* selection step) and then to apply a PLSR to the neighborhood 
(i.e. the *k* selected neighbors). 

Many variants of local PLSR pipelines can be built, depending essentially on *(a)* how are selected the 
neighborhoods and *(b)* the type of PLSR model implemented. One of these variants is the 
[kNN-LWPLSR algorithm](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/cem.3209)
described below. 

###### 1.2 Theory (summary)

The particularity of the kNN-LWPLSR algorithm is that it applies **locally weighted PLSR (LWPLSR)** 
(instead common PLSR) on each neighborhood. LWPLSR [20–22] is a particular case of **weighted PLSR (WPLSR)**. 
In WPLSR, a *n x 1* vector of weights *w = (w_1, w_2, … w_n)* is embedded into the PLSR algorithm. 
The PLS scores are computed by maximizing *w*-weighted squared covariances between the scores 
and the response variables. The MLR prediction equation is computed by regressing y on the scores 
using *w*-weighted least-squares. The *w*-weighting is also embedded in the centering and eventual 
scaling of the data. *Note:* in usual PLSR, a uniform weight, *1 / n*, is given to all the traing observations 
and therefore *w* can be removed from the the equations.

In LWPLSR, the weight vector *w* is computed from a decreasing function, say *f*, of dissimilarities (e.g. distances) 
between the *n* training observations and the observation to predict *x_new*. Closer is *x_i* to *x_new*, higher is the weight 
*w_i* in the PLSR equations and therefore its importance to the prediction. This is the same distance-based principle
as in the well-known locally weighted regression algorithm *LOESS* [23,24]. 

Compared to LWPLSR, kNN-LWPLSR simply adds a preliminary step: a neighborhood is selected around *x_new*, 
and then LWPLSR is applied to this neighborhood for prediction. kNN-LWPLSR can be viewed as 
a LWPLSR with a double weighting: a first binary weighting (0: *x_i* is not a neighbor, 1: *x_i* is a neighbor)
and a second weighting, intra-neighborhood, defined by function *f*. From an algorithmic point of view,
it is however much faster to compute kNN-LWPLSR than LWPLSR over the all dataset.  

#### 2. Function `lwplsr`

This section details the kNN-LWPLSR pipeline as defined in function 
[`lwplsr`](https://github.com/mlesnoff/Jchemo.jl/blob/master/src/lwplsr.jl) of package 
[Jchemo](https://github.com/mlesnoff/Jchemo.jl). 

###### 2.1 Keyword parameters 

Function `lwplsr` has several keyword parameters that can be specified. The full list is providen in the function help page  

```julia, eval = false
?lwplsr

  lwplsr(; kwargs...)
  lwplsr(X, Y; kwargs...)

  k-Nearest-Neighbours locally weighted partial least squares regression (kNN-LWPLSR).

    •  X : X-data (n, p).

    •  Y : Y-data (n, q).

  Keyword arguments:

    •  nlvdis : Number of latent variables (LVs) to consider in the global PLS used for the dimension reduction before computing the
       dissimilarities. If nlvdis = 0, there is no dimension reduction.

    •  metric : Type of dissimilarity used to select the neighbors and to compute the weights. Possible values are: :eucl (Euclidean distance),
       :mah (Mahalanobis distance).

    •  h : A scalar defining the shape of the weight function computed by function winvs. Lower is h, sharper is the function. See function
       winvs for details (keyword arguments criw and squared of winvs can also be specified here).

    •  k : The number of nearest neighbors to select for each observation to predict.

    •  tolw : For stabilization when very close neighbors.

    •  nlv : Nb. latent variables (LVs) for the local (i.e. inside each neighborhood) models.

    •  scal : Boolean. If true, (a) each column of the global X (and of the global Y if there is a preliminary PLS reduction dimension) is
       scaled by its uncorrected standard deviation before to compute the distances and the weights, and (b) the X and Y scaling is also done
       within each neighborhood (local level) for the weighted PLSR.

    •  verbose : Boolean. If true, predicting information are printed.

  [...]
```

and the default values of the parameters can be displayed by

```julia term = true
using Jchemo
@pars lwplsr
```

The main parameters to consider are: `nlvdis`, `metric`, `h`, `k` and `nlv`. Their meaning are are detailed below.

###### 2.2 Neighborhood computation 

A first step of the method is to choose if the dissimilarities between observations are computed after a dimension 
reduction of *X* or not. This is managed by parameter `nlvdis`
* If `nlvdis = 0`, there is not dimension reduction. 
* If `nlvdis > 0`, a preliminary global PLS with `nlvdis` LVs is done on the entire dataset *{X, y}* and the dissimilarities
    are computed on the resulting score matrix *T* (*n* x `nlvdis`).
    
Then the type of dissimilarities has to be chosen, with parameter `metric`. The available metrics are those 
proposed in funcion [`getknn`](https://github.com/mlesnoff/Jchemo.jl/blob/master/src/getknn.jl):

```julia, eval = false
•  metric : Type of distance used for the query. Possible values are :eucl (Euclidean), :mah (Mahalanobis), :sam (spectral angular
       distance), :cor (correlation distance).
```
Mahalanobis distance compouted on on 15-25 global *nlvdis* scores is often a good choice, but this is very dataset-dependent and other choices 
can also be as or more performant.

*Note:* If *X* has collinear columns (which is the case of NIRS data), Mahalanobis distance requires 
a preliminary dimension reduction since the inverse of the covariance matrix *cov(X)* can not be computed with stability.  

###### 2.2 Weighting function

Parameter `h` is the next paramater to set. Weight function [*f*](https://github.com/mlesnoff/Jchemo.jl/blob/master/src/winvs.jl) [8,21]  
has a negative exponential shape whose `h` defines its sharpness: lower is `h`, 
sharper is function *f* and therefore more the closest neighbors of *x_new* have importance in the LWPLSR fit. The case `h` = *Inf* is 
the unweighted situation (all the components of *w* are equal) corresponding to a kNN-PLSR.

Many alternative weight functions to *f* (e.g. bicube or tricube functions) could have been implemented.
Function *f* was chosen since it is versatile and easily tunable.

###### 2.3 Dimensions of the neighborhood and the local models 

The two final parameters to set are 
* `k`, the number of observations defining the neighborhood for each obserbation,
* `nlv`, the number of LVs considered in the local LWPLSR model. 
Note that if `k` is larger than the training size, kNN-LWPLSR reduces to LWLSR.

#### 3. Case study

Dataset [challenge2008](https://github.com/mlesnoff/JchemoData.jl/tree/main?tab=readme-ov-file#challenge2018) 
was built for the prediction challenge organized at congress Chemometrics2018 (Paris, January 2018). It consists in NIR spectra 
collected on various forages and feed samples. The variable to predict is the protein content.

###### 3.1 Data importation 

The dataset contains
* Object `X` (4075 *x* 680): The spectra, with wavelengths of 1120-2478 nm and a 2-nm step.
* Object `Y` (4075 *x* 4): Variable `conc` (protein concentration to predict) and other meta-data.

```julia, results = "hidden"
## Preliminary loading of packages
using Jchemo       # if not loaded before
using JchemoData   # a library of various benchmark datasets
using JLD2         # a Julia data format 
using CairoMakie   # making graphics 
using FreqTables   # utilities for frequency tables

path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, "data/challenge2018.jld2") 
@load db dat
@names dat

X = dat.X 
Y = dat.Y
y = Y.conc         # variable to predict
wlst = names(X)    # wavelengths
wl = parse.(Float64, wlst)
ntot, p = size(X)
```

```julia term = true
@head X
@head Y
```

```julia
freqtable(string.(Y.typ, '-', Y.label))
```

The spectra (random selection of 30 observations) can be plotted by 
```julia
plotsp(X, wl; size = (400, 300), nsamp = 30, xlabel = "Wavelength (nm)",
    ylabel = "Reflectance").f
```

###### 3.2 Data preprocessing

Two preprocessing steps are implemented to remove eventual non-informative physical effects in the spectra: a standard normal 
variation transformation (SNV), followed by a  Savitsky-Golay derivation/smoothing

```julia, results = "hidden"
model1 = snv()
model2 = savgol(npoint = 21, deriv = 2, degree = 3)
model = pip(model1, model2)
fit!(model, X)
Xp = transf(model, X) 
```

```julia
plotsp(Xp, wl; size = (400, 300), nsamp = 30, xlabel = "Wavelength (nm)").f
```

###### 3.3 Split training vs. test sets

Here the split of the total data is already provided in the dataset (variable `Y.test`), but the split could also be 
done *a posteriori* by *ad'hoc* sampling. 

```julia
freqtable(Y.test)
```

The final data are given by

```julia
s = Bool.(Y.test)
Xtrain = rmrow(Xp, s)
Ytrain = rmrow(Y, s)
ytrain = rmrow(y, s)
typtrain = rmrow(Y.typ, s)
Xtest = Xp[s, :]
Ytest = Y[s, :]
ytest = y[s]
typtest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = ntot, ntrain, ntest)
```

It is convenient to check that the test set is globally well represented by the training set, 
for instance using projections in a PCA score space 

```julia, echo = false
model = pcasvd(nlv = 15)
fit!(model, Xtrain)
Ttrain = model.fitm.T
Ttest = transf(model, Xtest)
T = vcat(Ttrain, Ttest)
group = vcat(repeat(["0-Train";], ntrain), repeat(["1-Test";], ntest))
colm = [:blue, (:red, .5)]
i = 1
plotxy(T[:, i], T[:, i + 1], group; size = (500, 400), color = colm, xlabel = "PC1", ylabel = "PC2").f
```

The plot *score (SD) vs. orthogonal (OD) distances* is even more powerful since it involves all dimensions of 
the PCA model

```julia, echo = false
model_sd = occsd() 
fit!(model_sd, model.fitm)
@names model_sd
sdtrain = model_sd.fitm.d
sdtest = predict(model_sd, Xtest).d
model_od = occod() 
fit!(model_od, model.fitm, Xtrain)
@names model_od
odtrain = model_od.fitm.d
odtest = predict(model_od, Xtest).d
f = Figure(size = (500, 400))
ax = Axis(f; xlabel = "SD", ylabel = "OD")
scatter!(ax, sdtrain.dstand, odtrain.dstand, label = "Train")
scatter!(ax, sdtest.dstand, odtest.dstand, color = (:red, .5), label = "Test")
hlines!(ax, 1; color = :grey, linestyle = :dash)
vlines!(ax, 1; color = :grey, linestyle = :dash)
axislegend(position = :rt)
f[1, 1] = ax
f
```

It is also usefull to check the representativity of the y-variable

```julia
summ(y, Y.test)
```

```julia, echo = false
f = Figure(size = (500, 400))
ax = Axis(f[1, 1]; xlabel = "Protein", ylabel = "Nb. observations")
hist!(ax, ytrain; bins = 50, label = "Train")
hist!(ax, ytest; bins = 50, label = "Test")
axislegend(position = :rt)
f
```

