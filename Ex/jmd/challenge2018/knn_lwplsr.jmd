---
title: Introduction to the kNN-LWPLSR algorithmn
weave_options:
  error: true
  wrap: true
  term: false
  out_width: "60%" 
---

The note introduces how to implement the 
[kNN-LWPLSR algorithm](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/cem.3209) 
with [Julia](https://julialang.org/) package 
[Jchemo](https://github.com/mlesnoff/Jchemo.jl).
The workflow examples are illustrated with dataset 
[challenge2008](https://github.com/mlesnoff/JchemoData.jl/tree/main?tab=readme-ov-file#challenge2018) 
on near-infrared spectroscopy (NIRS). *Note:* almost all the contents of this note can be diretly translated 
to the algorithm also provided in the R package 
[rchemo](https://cran.r-project.org/web/packages/rchemo/index.html).

kNN-LWPLSR is an efficient and versatile pipeline combining nearest neighborhood selection and partial least squared 
regression. kNN-LWPLSR is well suited when the predictive variables (*X*) are multicollinear and when 
heterogeneity in the data generates non-linear relations between *X* and the response variables
to predict (*Y*).  

#### 1. Preliminaries

###### 1.1 NIRS regression 

NIRS is a fast and nondestructive analytical method used in many agronomic contexts, for instance to 
evaluate the nutritive quality of forages. Basically, spectral data *X* (matrix of *n* observations and 
*p* wavelengths) are collected on samples of the material to study (e.g. forages) using a spectrometer, 
and targeted response variables *Y = {y1, …, yq}* (*q* vectors of *n* observations; e.g. chemical 
compositions) are measured precisely in laboratory. Regression models of *Y* on *X* are then fitted 
and used to predict the response variables from new spectral observations. 

Spectral data are known to be highly collinear in columns and, in general, matrix *X* is ill-conditioned. 
Regularization methods have to be implemented to solve the regression problem. A very popular regularization 
used for NIRS is the partial least squares regression (PLSR), which is one of the 
regression methods based on *latent variables* (LVs). PLSR starts by reducing the dimension of *X* 
(nb. columns) to a limited number *a << p* of orthogonal vectors *n x 1* maximizing the squared covariance 
with *Y*, and referred to as scores *T (n x a)*. Response *Y* is then regressed on scores *T* (latent variable) 
by a multiple linear regression (MLR). PLSR is in general very efficient when the relationship 
between *X* and *Y* is linear. The method is fast (in particular the *Dayal & McGregor kernel #1* algorithm [25]),
even for large data. The parameter to tune is the dimension of *T* (number of LVs).

For several years, agronomic databases (e.g. in feed, food or soils researches) tend to aggregate 
large numbers of samples of different natures or origins, bringing heterogeneity. This generates curvatures 
and/or clustering in the data that can alter the linear relation between *X* and *Y* and therefore the PLSR 
predictions. **Local PLSR** methodology is an easy approach that can turn out non-linearity in the data [4–7].  
The general principle is, for each new observation to predict (*x_new*), to do a pre-selection of *k* nearest 
neighbors of the observation (the *kNN* selection step) and then to apply a PLSR to the neighborhood 
(i.e. the *k* selected neighbors). 

Many variants of local PLSR pipelines can be built, depending essentially on *(a)* how are selected the 
neighborhoods and *(b)* the type of PLSR model implemented. One of these variants is the 
[kNN-LWPLSR algorithm](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/cem.3209)
described below. 

###### 1.2 Theory (summary)

The particularity of the kNN-LWPLSR algorithm is that it applies **locally weighted PLSR (LWPLSR)** 
(instead common PLSR) on each neighborhood. LWPLSR [20–22] is a particular case of **weighted PLSR (WPLSR)**. 
In WPLSR, a *n x 1* vector of weights *w = (w_1, w_2, … w_n)* is embedded into the PLSR algorithm. 
The PLS scores are computed by maximizing *w*-weighted squared covariances between the scores 
and the response variables. The MLR prediction equation is computed by regressing y on the scores 
using *w*-weighted least-squares. The *w*-weighting is also embedded in the centering and eventual 
scaling of the data. *Note:* in usual PLSR, a uniform weight, *1 / n*, is given to all the traing observations 
and therefore *w* can be removed from the the equations.

In LWPLSR, the weight vector *w* is computed from a decreasing function, say *f*, of dissimilarities 
(e.g. distances) between the *n* training observations and the observation to predict *x_new*. Closer is *x_i* 
to *x_new*, higher is the weight *w_i* in the PLSR equations and therefore its importance to the prediction. 
This is the same distance-based principle as in the well-known locally weighted regression algorithm *LOESS* [23,24]. 

Compared to LWPLSR, kNN-LWPLSR simply adds a preliminary step: a neighborhood is selected around *x_new*, 
and then LWPLSR is applied to this neighborhood for prediction. kNN-LWPLSR can be viewed as 
a LWPLSR with a double weighting: a first binary weighting (0: *x_i* is not a neighbor, 1: *x_i* is a neighbor)
and a second weighting, intra-neighborhood, defined by function *f*. From an algorithmic point of view,
it is however much faster to compute kNN-LWPLSR than LWPLSR over the all dataset.  

#### 2. Function `lwplsr`

This section details the kNN-LWPLSR pipeline as defined in function 
[`lwplsr`](https://github.com/mlesnoff/Jchemo.jl/blob/master/src/lwplsr.jl) of package 
[Jchemo](https://github.com/mlesnoff/Jchemo.jl). 

###### 2.1 Keyword parameters 

Function `lwplsr` has several keyword parameters that can be specified. The full list is providen in the function help page  

```julia, eval = false
?lwplsr

  lwplsr(; kwargs...)
  lwplsr(X, Y; kwargs...)

  k-Nearest-Neighbours locally weighted partial least squares regression (kNN-LWPLSR).

    •  X : X-data (n, p).

    •  Y : Y-data (n, q).

  Keyword arguments:

    •  nlvdis : Number of latent variables (LVs) to consider in the global PLS used for the dimension reduction before computing the
       dissimilarities. If nlvdis = 0, there is no dimension reduction.

    •  metric : Type of dissimilarity used to select the neighbors and to compute the weights. Possible values are: :eucl (Euclidean distance),
       :mah (Mahalanobis distance).

    •  h : A scalar defining the shape of the weight function computed by function winvs. Lower is h, sharper is the function. See function
       winvs for details (keyword arguments criw and squared of winvs can also be specified here).

    •  k : The number of nearest neighbors to select for each observation to predict.

    •  tolw : For stabilization when very close neighbors.

    •  nlv : Nb. latent variables (LVs) for the local (i.e. inside each neighborhood) models.

    •  scal : Boolean. If true, (a) each column of the global X (and of the global Y if there is a preliminary PLS reduction dimension) is
       scaled by its uncorrected standard deviation before to compute the distances and the weights, and (b) the X and Y scaling is also done
       within each neighborhood (local level) for the weighted PLSR.

    •  verbose : Boolean. If true, predicting information are printed.

  [...]
```

and the default values of the parameters can be displayed by

```julia term = true
using Jchemo
@pars lwplsr
```

The main parameters to consider are: `nlvdis`, `metric`, `h`, `k` and `nlv`. Their meaning are are detailed below.

###### 2.2 Neighborhood computation 

A first step of the method is to choose if the dissimilarities between observations are computed after a dimension 
reduction of *X* or not. This is managed by parameter `nlvdis`
* If `nlvdis = 0`, there is not dimension reduction. 
* If `nlvdis > 0`, a preliminary global PLS with `nlvdis` LVs is done on the entire dataset *{X, y}* and the dissimilarities
    are computed on the resulting score matrix *T* (*n* x `nlvdis`).
    
Then the type of dissimilarities has to be chosen, with parameter `metric`. The available metrics are those 
proposed in funcion [`getknn`](https://github.com/mlesnoff/Jchemo.jl/blob/master/src/getknn.jl):

```julia, eval = false
•  metric : Type of distance used for the query. Possible values are :eucl (Euclidean), :mah (Mahalanobis), :sam (spectral angular
       distance), :cor (correlation distance).
```
Mahalanobis distance compouted on on 15-25 global *nlvdis* scores is often a good choice, but this is very dataset-dependent and other choices 
can also be as or more performant.

*Note:* If *X* has collinear columns (which is the case of NIRS data), Mahalanobis distance requires 
a preliminary dimension reduction since the inverse of the covariance matrix *cov(X)* can not be computed with stability.  

###### 2.2 Weighting function

Parameter `h` is the next paramater to set. Weight function [*f*](https://github.com/mlesnoff/Jchemo.jl/blob/master/src/winvs.jl) [8,21]  
has a negative exponential shape whose `h` defines its sharpness: lower is `h`, 
sharper is function *f* and therefore more the closest neighbors of *x_new* have importance in the LWPLSR fit. The case `h` = *Inf* is 
the unweighted situation (all the components of *w* are equal) corresponding to a kNN-PLSR.

Many alternative weight functions to *f* (e.g. bicube or tricube functions) could have been implemented.
Function *f* was chosen since it is versatile and easily tunable.

###### 2.3 Dimensions of the neighborhood and the local models 

The two final parameters to set are 
* `k`, the number of observations defining the neighborhood for each obserbation,
* `nlv`, the number of LVs considered in the local LWPLSR model. 
Note that if `k` is larger than the training size, kNN-LWPLSR reduces to LWLSR.

#### 3. Case study

Dataset [challenge2008](https://github.com/mlesnoff/JchemoData.jl/tree/main?tab=readme-ov-file#challenge2018) 
was built for the prediction challenge organized at congress Chemometrics2018 (Paris, January 2018). It consists in NIR spectra 
collected on various forages and feed samples. The variable to predict is the protein content.

###### 3.1 Data importation 

The dataset contains
* Object `X` (4075 *x* 680): The spectra, with wavelengths of 1120-2478 nm and a 2-nm step.
* Object `Y` (4075 *x* 4): Variable `conc` (protein concentration to predict) and other meta-data.

```julia, results = "hidden"
## Preliminary loading of packages
using Jchemo       # if not loaded before
using JchemoData   # a library of various benchmark datasets
using JLD2         # a Julia data format 
using CairoMakie   # making graphics 
using FreqTables   # utilities for frequency tables

path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, "data/challenge2018.jld2") 
@load db dat
@names dat

X = dat.X 
Y = dat.Y
y = Y.conc         # variable to predict
wlst = names(X)    # wavelengths
wl = parse.(Float64, wlst)
ntot, p = size(X)
```

```julia term = true
@head X
@head Y
```

```julia
freqtable(string.(Y.typ, '-', Y.label))
```

The spectra (random selection of 30 observations) can be plotted by 
```julia
plotsp(X, wl; size = (400, 300), nsamp = 30, xlabel = "Wavelength (nm)",
    ylabel = "Reflectance").f
```

###### 3.2 Data preprocessing

Two preprocessing steps are implemented to remove eventual non-informative physical effects in the spectra: a standard normal 
variation transformation (SNV), followed by a  Savitsky-Golay derivation/smoothing

```julia, results = "hidden"
model1 = snv()
model2 = savgol(npoint = 21, deriv = 2, degree = 3)
model = pip(model1, model2)
fit!(model, X)
Xp = transf(model, X) 
```

```julia
plotsp(Xp, wl; size = (400, 300), nsamp = 30, xlabel = "Wavelength (nm)").f
```

###### 3.3 Split training *vs.* test sets

Here the split of the total data is already provided in the dataset (variable `Y.test`), but the split could also be 
done *a posteriori* by *ad'hoc* sampling. 

```julia
freqtable(Y.test)
```

The final data are given by

```julia
s = Bool.(Y.test)
Xtrain = rmrow(Xp, s)
Ytrain = rmrow(Y, s)
ytrain = rmrow(y, s)
typtrain = rmrow(Y.typ, s)
Xtest = Xp[s, :]
Ytest = Y[s, :]
ytest = y[s]
typtest = Y.typ[s]
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = ntot, ntrain, ntest)
```

It is convenient to check that the test set is globally well represented by the training set, 
for instance using projections in a PCA score space 

```julia, echo = false
model = pcasvd(nlv = 15)
fit!(model, Xtrain)
Ttrain = model.fitm.T
Ttest = transf(model, Xtest)
T = vcat(Ttrain, Ttest)
group = vcat(repeat(["0-Train";], ntrain), repeat(["1-Test";], ntest))
colm = [:blue, (:red, .5)]
i = 1
plotxy(T[:, i], T[:, i + 1], group; size = (500, 400), color = colm, xlabel = "PC1", ylabel = "PC2").f
```

The plot *score (SD) vs. orthogonal (OD) distances* is even more powerful since it involves all dimensions of 
the PCA model

```julia, echo = false
model_sd = occsd() 
fit!(model_sd, model.fitm)
@names model_sd
sdtrain = model_sd.fitm.d
sdtest = predict(model_sd, Xtest).d
model_od = occod() 
fit!(model_od, model.fitm, Xtrain)
@names model_od
odtrain = model_od.fitm.d
odtest = predict(model_od, Xtest).d
f = Figure(size = (500, 400))
ax = Axis(f; xlabel = "SD", ylabel = "OD")
scatter!(ax, sdtrain.dstand, odtrain.dstand, label = "Train")
scatter!(ax, sdtest.dstand, odtest.dstand, color = (:red, .5), label = "Test")
hlines!(ax, 1; color = :grey, linestyle = :dash)
vlines!(ax, 1; color = :grey, linestyle = :dash)
axislegend(position = :rt)
f[1, 1] = ax
f
```

It is also usefull to check the representativity of the y-variable

```julia
summ(y, Y.test)
```

```julia, echo = false
f = Figure(size = (500, 400))
ax = Axis(f[1, 1]; xlabel = "Protein", ylabel = "Nb. observations")
hist!(ax, ytrain; bins = 50, label = "Train")
hist!(ax, ytest; bins = 50, label = "Test")
axislegend(position = :rt)
f
```

###### 3.4 Model tuning 

A usual approach of model tuning is to do a grid search (evaluate the model performance over a grid of 
parameter combinations) using a sampling design that splits the training data to several calibration 
*vs.* validation sets. The popular K-Fold cross-validation (CV) is such a sampling design. Nevertheless, 
CV requires to predict all of the training observations (here *ntrain* = 3,701 obs.) for each parameter combination, 
which can be too time consuming for local PLSR and large datasets. A slighter is to split the training set
to a calibration set and a validation set, and to run the grid search on the validation set. This is what is done in this 
note, using the [`gridscore`](https://github.com/mlesnoff/Jchemo.jl/blob/master/src/gridscore.jl) function. 

Below, the validation set is selected by systematic sampling over the rank of the *y* observations (but 
other sampling designs, e.g. random,  could be chosen)

```julia
nval = 300
s = sampsys(ytrain, nval)
Xcal = Xtrain[s.train, :]
ycal = ytrain[s.train]
Xval = Xtrain[s.test, :]
yval = ytrain[s.test]
ncal = ntrain - nval 
(ntot = ntot, ntrain, ntest, ncal, nval)
```

Then the grid of parameters is built by

```julia 
nlvdis = [15; 20; 25] ; metric = [:mah] 
h = [1; 2; 4; 6; Inf]
k = [150; 200; 300; 400; 500; 700; 1000]  
nlv = 0:15
pars = mpar(nlvdis = nlvdis, metric = metric, h = h, k = k)
```

```julia 
length(pars[1])  # nb. parameter combinations considered
```

Consider the performance criterion as the RMSEP computed on the validation test

```julia
model = lwplsr()
res = gridscore(model, Xcal, ycal, Xval, yval; score = rmsep, pars, nlv, verbose = false)
@head res   # first rows of the result table
```

that gives graphically

```julia
group = string.("nlvdis=", res.nlvdis, ", h=", res.h, ", k=", res.k) 
plotgrid(res.nlv, res.y1, group; step = 1, xlabel ="Nb. LVs", ylabel = "RMSEP_Val").f
```

###### 3.5 Final predictions

The final model can be defined by selecting the best parameters combination

```julia
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
```

and the the final prediction on the test are given by

```julia
model = lwplsr(nlvdis = res.nlvdis[u], metric = res.metric[u], h = res.h[u], 
    k = res.k[u], nlv = res.nlv[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred 
@head pred 
```

Summary of the predictions

```julia
mse(pred, ytest)
```

```julia
rmsep(pred, ytest)
```

```julia
plotxy(pred, ytest; color = (:red, .5), bisect = true, xlabel = "Predictions", 
    ylabel = "Observed test data", title = "Protein concentration (%)").f
```

#### 4. Related references

* Andersson M. A comparison of nine PLS1 algorithms. J Chemom. 2009;23(10):518-529. 
doi:10.1002/cem.1248.

* Cleveland, W. S. (1979). Robust locally weighted regression and smoothing scatterplots. 
Journal of the American statistical association, 74(368), 829-836. 
DOI: 10.1080/01621459.1979.10481038

* Cleveland, W. S., & Devlin, S. J. (1988). Locally weighted regression: an approach to 
regression analysis by local fitting. Journal of the American statistical association, 83(403), 
596-610. DOI:10.1080/01621459.1988.10478639

* Davrieux F, Dufour D, Dardenne P, et al. LOCAL regression algorithm improves near infrared 
spectroscopy predictions when the target constituent evolves in breeding populations. 
J Infrared Spectrosc. 2016;24(2):109. doi:10.1255/jnirs.1213

* Davrieux F, Dufour D, Dardenne P, et al. LOCAL regression algorithm improves near infrared 
spectroscopy predictions when the target constituent evolves in breeding populations. 
J Infrared Spectrosc. 2016;24(2):109. doi:10.1255/jnirs.1213

* Dayal BS, MacGregor JF. Improved PLS algorithms. J Chemom. 1997;11(1):73-85. 
doi:10.1002/(SICI)1099-128X(199701)11:1<73::AID-CEM435>3.0.CO;2-#.

* Höskuldsson A. PLS regression methods. J Chemom. 1988;2(3):211-228. doi:10.1002/cem.1180020306.

* Kim S, Kano M, Nakagawa H, Hasebe S. Estimation of active pharmaceutical ingredients content 
using locally weighted partial least squares and statistical wavelength selection. 
Int J Pharm. 2011;421(2):269-274. doi:10.1016/j.ijpharm.2011.10.007

* Lesnoff, M., Metz, M., Roger, J.-M., 2020. Comparison of locally weighted PLS strategies 
for regression and discrimination on agronomic NIR data. Journal of Chemometrics n/a, e3209. 
https://doi.org/10.1002/cem.3209

* Lesnoff, M., 2024. Averaging a local PLSR pipeline to predict chemical compositions and 
nutritive values of forages and feed from spectral near infrared data. Chemometrics 
and Intelligent Laboratory Systems 244, 105031. https://doi.org/10.1016/j.chemolab.2023.105031

* Manne R. Analysis of two partial-least-squares algorithms for multivariate calibration. 
Chemom Intell Lab Syst. 1987;2(1-3):187-197. doi:10.1016/0169-7439(87)80096-5.
 
* Schaal, S., Atkeson, C.G., Vijayakumar, S., 2002. Scalable Techniques from Nonparametric 
Statistics for Real Time Robot Learning. Applied Intelligence 17, 49–60. 
https://doi.org/10.1023/A:1015727715131

* Shenk J, Westerhaus M, Berzaghi P. Investigation of a LOCAL calibration procedure 
for near infrared instruments. J Infrared Spectrosc. 1997;5(1):223. doi:10.1255/jnirs.115

* Sicard E, Sabatier R. Theoretical framework for local PLS1 regression, and application 
to a rainfall data set. Comput Stat Data Anal. 2006;51(2):1393-1410. doi:10.1016/j.csda.2006.05.002.

* Wold H. Nonlinear iterative partial least squares (NIPALS) modeling: some current developments. 
In: Multivariate Analysis II. Wright State University, Dayton, Ohio, USA.  June 19–24, 1972. 
New York : Academic Press: Krishnaiah , P. R.; 1973:383 – 407.

* Wold S, Sjöström M, Eriksson L. PLS-regression: a basic tool of chemometrics. 
Chemom Intell Lab Syst. 2001;58(2):109-130. doi:10.1016/S0169-7439(01)00155-1.

* Yoshizaki R, Kano M, Tanabe S, Miyano T. Process Parameter Optimization based on LW-PLS 
in Pharmaceutical Granulation Process∗∗This work was partially supported by Japan Society for the 
Promotion of Science (JSPS), Grant-in-Aid for Scientific Research (C) 24560940. IFAC-Pap. 
2015;48(8):303-308. doi:10.1016/j.ifacol.2015.08.198.

* Zhang X, Kano M, Li Y. Locally weighted kernel partial least squares regression based on 
sparse nonlinear features for virtual sensing of nonlinear time-varying processes. 
Comput Chem Eng. 2017;104:164-171. doi:10.1016/j.compchemeng.2017.04.014.


