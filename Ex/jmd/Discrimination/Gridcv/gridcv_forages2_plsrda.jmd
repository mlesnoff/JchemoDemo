---
title: gridcv - forages2 - Plsrda 
weave_options:
  error: true
  wrap: false
  term: false
  out_width: 60%
  out_height: 30%
---

```julia
using Jchemo, JchemoData
using JLD2, CairoMakie
using FreqTables 
```

#### Data importation

```julia
path_jdat = dirname(dirname(pathof(JchemoData)))
db = joinpath(path_jdat, "data/forages2.jld2") 
@load db dat
@names dat
```

```julia
X = dat.X 
@head X 
```

```julia
Y = dat.Y
@head Y
```

```julia
y = Y.typ   # response variable (class membership)
test = Y.test
tab(y)
```

```julia
freqtable(y, test)
```

```julia
wlst = names(X)
wl = parse.(Int, wlst)
#plotsp(X, wl; xlabel = "Wavelength (nm)", ylabel = "Absorbance").f
```

**Note:**: X-data are already preprocessed (SNV + Savitsky-Golay 2nd deriv).

#### Split Tot to Train/Test

The model is fitted on **Train**, and the generalization error is estimated on **Test**. In this example, **Train** is 
already defined in variable `typ` of the dataset, and **Test** is defined by the remaining samples. But **Tot** could 
also be split *a posteriori*, for instance by sampling (random, systematic or any other designs). See for instance 
functions `samprand`, `sampsys`, etc.

```julia
s = Bool.(test)
Xtrain = rmrow(X, s)
ytrain = rmrow(y, s)
Xtest = X[s, :]
ytest = y[s]
ntot = nro(X)
ntrain = nro(Xtrain)
ntest = nro(Xtest)
(ntot = ntot, ntrain, ntest)
```

```julia
tab(ytrain)
```

```julia
tab(ytest)
```

#### Replicated K-fold CV

```julia
K = 3     # nb. folds (segments)
rep = 25  # nb. replications
segm = segmkf(ntrain, K; rep = rep)
```

```julia
nlv = 0:30
model = plsrda()
rescv = gridcv(model, Xtrain, ytrain; segm, score = errp, nlv)
@names rescv 
res = rescv.res
res_rep = rescv.res_rep
```

```julia
plotgrid(res.nlv, res.y1; step = 2, xlabel = "Nb. LVs", ylabel = "ERRP-CV").f
```

```julia
f, ax = plotgrid(res.nlv, res.y1; step = 2, xlabel = "Nb. LVs", ylabel = "ERRP-CV")
for i = 1:rep, j = 1:K
    zres = res_rep[res_rep.rep .== i .&& res_rep.segm .== j, :]
    lines!(ax, zres.nlv, zres.y1; color = (:grey, .2))
end
lines!(ax, res.nlv, res.y1; color = :red, linewidth = 1)
f
```

**Specifying argument** `prior`

This is recommended if classes are highly unbalanced.

```julia
prior = [:unif]  
pars = mpar(prior = prior)
nlv = 0:30
model = plsrda()
res = gridcv(model, Xtrain, ytrain; segm, score = merrp, pars, nlv).res
```

**Selection of the best parameter combination**

```julia
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
```

#### Final prediction (Test) using the optimal model

```julia
model = plsrda(nlv = res.nlv[u], prior = res.prior[u])
fit!(model, Xtrain, ytrain)
pred = predict(model, Xtest).pred
```

**Generalization error**

```julia
errp(pred, ytest)
```

```julia
merrp(pred, ytest)
```

```julia
cf = conf(pred, ytest)
@names cf
```

```julia
cf.cnt
```

```julia
cf.pct
```

